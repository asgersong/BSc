\chapter{Literature Review}

\section{Overview of Large Language Models}
    The evolution of LLMs can be traced back to earlier models of machine learning that attempted to process and understand language. However, it was the introduction of models like Google's BERT (Bidirectional Encoder Representations from Transformers) and OpenAI's GPT (Generative Pre-trained Transformer) series that marked a significant leap in the capabilities of language models. Each iteration of these models has brought about improvements in understanding context, generating text, and general language comprehension, culminating in state-of-the-art models that are capable of writing essays, composing poetry, and even generating code.


\section{Previous Studies on LLM Compression and Optimization}

    \begin{itemize}
        \item A Survey on Model Compression for Large Language Models
        \item The complete guide to LLM compression
    \end{itemize}