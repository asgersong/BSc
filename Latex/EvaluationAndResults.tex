\chapter{Evaluation and Results}

\section{Curriculum Effectiveness}

\section{Chatbot Performance and Usefulness}

\section{Compression and Optimization of LLM}
    \subsection{Methodology Applied}

    \subsection{Results and Analysis}
    The evaluation of the compressing the BART-Base model using low-rank approximation reveals the following insights:
    
    \textbf{Model Size Reduction:} The low-rank approximation technique achieves a significant reduction in the total number of parameters, demonstrating its effectiveness in compressing the model.
    
    \textbf{Computational Efficiency:} The compressing the model shows that the \\ \texttt{eval\_runtime} is almost linear with the rank \(r\), indicating the potential for computational savings.
    
    \textbf{Cosine Similarity:}

    \textbf{ROUGE Scores:} The compressed model maintains competitive ROUGE scores compared to the baseline BART-Base model, indicating that the low-rank approximation does not significantly impact the model's summarization performance.