\babel@toc {english}{}\relax 
\contentsline {chapter}{\chapternumberline {1}Introduction}{7}{chapter.1}%
\contentsline {chapter}{\chapternumberline {2}Literature Review}{9}{chapter.2}%
\contentsline {section}{\numberline {2.1}Overview of Large Language Models}{9}{section.2.1}%
\contentsline {section}{\numberline {2.2}Previous Studies on LLM Compression and Optimization}{9}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Low-Memory Optimization}{10}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Optimization by prompting}{10}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Low-Rank Adaptation (LoRA)}{11}{subsection.2.2.3}%
\contentsline {paragraph}{Theoretical Foundation}{11}{subsection.2.2.3}%
\contentsline {paragraph}{Advantages}{11}{equation.2.2.1}%
\contentsline {paragraph}{Practical Implementation}{11}{equation.2.2.1}%
\contentsline {section}{\numberline {2.3}Evaluating Summarization with ROUGE}{12}{section.2.3}%
\contentsline {paragraph}{ROUGE Metrics}{12}{section.2.3}%
\contentsline {paragraph}{Application and Relevance}{12}{section.2.3}%
\contentsline {paragraph}{Significance in LLM Research}{12}{section.2.3}%
\contentsline {chapter}{\chapternumberline {3}Theoretical Foundations}{13}{chapter.3}%
\contentsline {section}{\numberline {3.1}Linear Algebra in Deep Learning}{13}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Vectors, Matrices, and Tensors}{13}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Matrix Operations}{13}{subsection.3.1.2}%
\contentsline {subsubsection}{Flops}{14}{equation.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}Singular Value Decomposition}{14}{subsection.3.1.3}%
\contentsline {subsection}{\numberline {3.1.4}Neural Networks in Deep Learning}{14}{subsection.3.1.4}%
\contentsline {subsubsection}{Architecture of Neural Networks}{14}{section*.5}%
\contentsline {subsubsection}{Learning Process}{14}{equation.3.1.4}%
\contentsline {subsubsection}{Optimization and Regularization}{15}{equation.3.1.5}%
\contentsline {section}{\numberline {3.2}Understanding LLMs}{15}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}What are LLMs?}{15}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Architecture of LLMs}{15}{subsection.3.2.2}%
\contentsline {subsubsection}{Encoder}{15}{figure.3.1}%
\contentsline {subsubsection}{Decoder}{16}{figure.3.1}%
\contentsline {subsubsection}{Attention}{16}{figure.3.1}%
\contentsline {subsubsection}{Multi-head attention}{17}{equation.3.2.6}%
\contentsline {subsubsection}{Why Transformers?}{17}{equation.3.2.8}%
\contentsline {subsection}{\numberline {3.2.3}Training and Fine-Tuning}{17}{subsection.3.2.3}%
\contentsline {chapter}{\chapternumberline {4}Methodology}{19}{chapter.4}%
\contentsline {section}{\numberline {4.1}Educational Synergy Development}{19}{section.4.1}%
\contentsline {section}{\numberline {4.2}Software Engineering Application}{19}{section.4.2}%
\contentsline {section}{\numberline {4.3}LLM Optimization}{19}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Optimization Techniques for Large Language Models}{19}{subsection.4.3.1}%
\contentsline {subsubsection}{Model Pruning}{19}{section*.6}%
\contentsline {subsubsection}{Parameter Sharing}{19}{section*.6}%
\contentsline {subsubsection}{Focus on Low-Rank Approximation}{20}{section*.6}%
\contentsline {subsection}{\numberline {4.3.2}Metrics for Evaluation}{20}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Case Study: Low-Rank Approximation}{20}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Low-Rank Approximation Technique}{21}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Implementation Steps}{21}{subsection.4.4.2}%
\contentsline {chapter}{\chapternumberline {5}Implementation}{23}{chapter.5}%
\contentsline {section}{\numberline {5.1}Curriculum Component Implementation}{23}{section.5.1}%
\contentsline {section}{\numberline {5.2}Chatbot Development}{23}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Tools and Libraries Used}{23}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Integration of LLM}{23}{subsection.5.2.2}%
\contentsline {chapter}{\chapternumberline {6}Evaluation and Results}{25}{chapter.6}%
\contentsline {section}{\numberline {6.1}Curriculum Effectiveness}{25}{section.6.1}%
\contentsline {section}{\numberline {6.2}Chatbot Performance and Usefulness}{25}{section.6.2}%
\contentsline {section}{\numberline {6.3}Compression and Optimization of LLM}{25}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}Methodology Applied}{25}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Results and Analysis}{25}{subsection.6.3.2}%
\contentsline {chapter}{\chapternumberline {7}Discussion}{27}{chapter.7}%
\contentsline {section}{\numberline {7.1}Interpretation of Results}{27}{section.7.1}%
\contentsline {section}{\numberline {7.2}Theoretical and Practical Implications}{27}{section.7.2}%
\contentsline {section}{\numberline {7.3}Limitations and Challenges}{27}{section.7.3}%
\contentsline {chapter}{\chapternumberline {8}Conclusion and Future Work}{29}{chapter.8}%
\contentsline {section}{\numberline {8.1}Summary of Key Findings}{29}{section.8.1}%
\contentsline {section}{\numberline {8.2}Contributions to the Field}{29}{section.8.2}%
\contentsline {section}{\numberline {8.3}Recommendations for Future Research}{29}{section.8.3}%
\contentsline {chapter}{Bibliography}{31}{section*.8}%
