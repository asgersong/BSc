\chapter{Methodology}

\section{Educational Synergy Development}

\section{Software Engineering Application}

\section{LLM Optimization}
    Optimizing Large Language Models (LLMs) for efficiency and performance without compromising their effectiveness is a critical area of research in the field of artificial intelligence. Various techniques have been developed to address this challenge, each employing unique strategies to reduce computational resources, decrease model size, and maintain, if not improve, the model's performance. This section explores the general methodologies applied in the optimization of LLMs, focusing particularly on model compression techniques. Among these, Low-Rank Adaptation (LoRA) stands out as a significant innovation, offering a balance between model efficiency and task performance.

    \subsection{Optimization Techniques for Large Language Models}
        Optimization strategies for Large Language Models (LLMs) are crucial for improving computational efficiency and model efficacy. These strategies are generally divided into two primary types: model pruning and parameter sharing.

        \subsubsection{Model Pruning}
        Model pruning aims to reduce the model's complexity and size effectively without substantial loss in performance. It involves systematically eliminating parameters or connections within the model that are least consequential to the output, thereby enhancing operational efficiency and making the model more adaptable for use in environments with limited resources.

        \subsubsection{Parameter Sharing}
        Conversely, parameter sharing utilizes the model's existing parameters across various parts of the model or different tasks. This method optimizes the use of the modelâ€™s capacity, enabling multifunctional performance without an increase in parameter count.

        \subsubsection{Focus on Low-Rank Approximation}
        This thesis will specifically focus on model pruning techniques, with a particular emphasis on low-rank approximation. This approach approximates large matrices or tensors with ones of a lower rank, reducing the number of parameters and computational demands while preserving the model's critical information processing capabilities.

    \subsection{Metrics for Evaluation}
        Evaluating the effectiveness of optimization techniques like LoRA involves several key metrics:
        
        \textbf{Model Size Reduction:} A primary metric is the reduction in the total number of parameters, indicating the efficiency of the compression technique.
        
        \textbf{Inference Speed:} The impact on the model's inference latency is critical, especially for applications requiring real-time responses.

        \textbf{Cosine Similarity:} In the context of model tuning and adaptation, cosine similarity can serve as a measure of how well the optimized model maintains the semantic properties of the original. It is especially useful in assessing the quality of embeddings and the alignment between the compressed and original model outputs.
        
        \textbf{Performance Retention:} The maintenance of task-specific performance, measured through metrics such as accuracy, F1 score, or ROUGE scores for language tasks, is essential to ensure the utility of the compressed model.
        
        \textbf{Computational Efficiency:} The reduction in computational resources required for training and inference reflects the practical benefits of the optimization technique.

        These metrics provide a comprehensive framework for assessing the trade-offs involved in LLM optimization, guiding the development and application of techniques like LoRA in enhancing the practical utility of state-of-the-art language models.

\section{Case Study: Low-Rank Approximation}
    To illustrate the application of low-rank approximation in compressing LLMs, we consider Facebook's the BART-Base model as a case study. BART is a transformer-based LLM that has been widely used for various natural language processing tasks. By applying low-rank approximation to the attention weight matrices of the BART-base model, we aim to reduce its parameter size while maintaining its performance on a summarization task.

    The choice to apply low-rank approximation specifically to the attention matrices stems from their pivotal role in the transformer architecture. These matrices, which help the model assess the relevance of different words within the input data, tend to be large and often encapsulate redundant information (\cite{aghajanyan2020intrinsic}). By reducing the dimensionality of these matrices, we preserve the model's ability to perform complex relational reasoning with less computational overhead, thus maintaining efficacy in summarization tasks while enhancing efficiency.
    \subsection{Low-Rank Approximation Technique}
        Low-rank approximation is a matrix factorization technique that decomposes a given matrix into the product of some lower-dimensional matrices. In the context of LLMs, low-rank approximation can be applied to the attention weight matrices to reduce the number of parameters while preserving the model's representational capacity.

    \subsection{Implementation Steps}
        The implementation of low-rank approximation for BART involves the following steps:
        
        \textbf{Singular Value Decomposition (SVD):} The attention weight matrices (Key, Query, Value, and Output) of the model are decomposed using SVD to obtain the low-rank approximation.
        
        \textbf{Custom Layer Implementation:} A custom layer is implemented to replace the original attention layers with the low-rank approximation, reducing the model's parameter size.
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.4\textwidth]{figs/insert.png}
                \caption{Attention layers replaced with low-rank approximation}
                \label{fig:lora_implementation}
            \end{figure}
        \textbf{Evaluation Metrics:} The compressed model is evaluated based on ROUGE scores (\cite{lin-2004-rouge}), cosine similarity, metrics such as model size reduction, computational efficiency, cosine similarity, and task-specific performance on a summarization task using the Samsum dataset (\cite{gliwa-etal-2019-samsum}).
        
        \textbf{Rank Selection:} The rank of the approximation is chosen based on the observed spectrum decay of the ROUGE-scores, ensuring a balance between speed and task performance retention.
    